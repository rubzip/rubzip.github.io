---
title: "Transformer Implementation"
excerpt: 'Implementation of transformer encoder-decoder model based on paper <a href="https://arxiv.org/abs/1706.03762" target="_blank">Attention is all you need</a>. <a href="https://github.com/rubzip/Transformers-Implementation" target="_blank">Code</a>.'
collection: portfolio
---

Implementation of transformer encoder-decoder model based on paper <a href="https://arxiv.org/abs/1706.03762" target="_blank">Attention is all you need</a>.

It includes (all built from scratch on pyTorch):
- **Encoder and Decoder layers**  
- **Multi-Head Attention** and **Scaled Dot-Product Attention**  
- **Add & Norm layer**  
- **Position-wise Feed-Forward Networks**  
- **Sinusoidal Positional Encodings**  
All of them were implemented following paper..


Repository: [https://github.com/rubzip/Transformers-Implementation](https://github.com/rubzip/Transformers-Implementation)

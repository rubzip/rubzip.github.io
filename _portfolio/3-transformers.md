---
title: "Transformer Implementation"
excerpt: 'Implementation of paper "Attention is all you need". <a href="https://github.com/rubzip/Transformers-Implementation" target="_blank">Code</a>'
collection: portfolio
---

It includes:
- **Encoder and Decoder stacks** built from scratch  
- **Multi-Head Attention** and **Scaled Dot-Product Attention**  
- **Add & Norm layers** with residual connections  
- **Position-wise Feed-Forward Networks**  
- **Sinusoidal Positional Encodings**  

Repository: [https://github.com/rubzip/Transformers-Implementation](https://github.com/rubzip/Transformers-Implementation)
